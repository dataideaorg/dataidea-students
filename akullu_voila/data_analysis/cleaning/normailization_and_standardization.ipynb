{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e314b07-1d2f-43cf-b259-0ad8659f84af",
   "metadata": {},
   "source": [
    "## Data Normalization and Standardization\n",
    "\n",
    "#### Introduction:\n",
    "In data analysis and machine learning, preprocessing steps such as data normalization and standardization are crucial for improving the performance and interpretability of models.\n",
    "This Jupyter Notebook provides an overview of the importance of data normalization and standardization in preparing data for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b267c9-ad35-4e32-8ed2-9370f15f910c",
   "metadata": {},
   "source": [
    "#### Importance:\n",
    "\n",
    "1. Data Normalization:\n",
    "   - Uniform Scaling: Ensures all features are scaled to a similar range, preventing dominance by features with larger scales.\n",
    "   - Improved Convergence: Facilitates faster convergence in optimization algorithms by making the loss surface more symmetric.\n",
    "   - Interpretability: Easier interpretation as values are on a consistent scale, aiding in comparison and understanding of feature importance.\n",
    "\n",
    "\n",
    "2. Data Standardization:\n",
    "   - Mean Centering: Transforms data to have a mean of 0 and a standard deviation of 1, simplifying interpretation of coefficients in linear models.\n",
    "   - Handling Different Scales: Useful when features have different scales or units, making them directly comparable.\n",
    "   - Reducing Sensitivity to Outliers: Less affected by outliers compared to normalization, leading to more robust models.\n",
    "   - Maintaining Information: Preserves relative relationships between data points without altering the distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc35ed7-0dac-417e-aa48-d3cc6e2c3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization without libraries:\n",
    "def minMaxScaling(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    scaled_data = []\n",
    "    for value in data:\n",
    "        scaled = (value - min_val) / (max_val - min_val)\n",
    "        scaled_data.append(scaled)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e3edee-617f-4f35-ba08-5e8b35328f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data (Min-Max Scaling): [0.0, 0.25, 0.5, 0.75, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "data = [10, 20, 30, 40, 50]\n",
    "normalized_data = minMaxScaling(data)\n",
    "print(\"Normalized data (Min-Max Scaling):\", normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcaf240-2f90-47ba-862f-90b322135345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Standardization without libraries:\n",
    "def zScoreNormalization(data):\n",
    "    mean = sum(data) / len(data)\n",
    "    variance = sum((x - mean) ** 2 for x in data) / len(data)\n",
    "    std_dev = variance ** 0.5\n",
    "    standardized_data = [(x - mean) / std_dev for x in data]\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f142e-6f09-45c2-977c-8b19d3fdb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "data = [10, 20, 30, 40, 50]\n",
    "standardized_data = z_score_normalization(data)\n",
    "print(\"Standardized data (Z-Score Normalization):\", standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3320891-7118-425d-8fbb-c3a4742ace32",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Both data normalization and standardization are critical preprocessing steps in data analysis and machine learning.\n",
    "Their importance lies in improving model performance, interpretability, and robustness while preserving the underlying data relationships.\n",
    "The choice between normalization and standardization depends on the specific characteristics of the data and modeling requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
